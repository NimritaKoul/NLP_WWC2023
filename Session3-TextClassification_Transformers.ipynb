{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731748e3",
   "metadata": {},
   "source": [
    "# Natural Language Processing using Python\n",
    "#### This  4 hour workshop will introduce the audience to the field of Natural Language Processing,  we will cover the fundamental tasks in NLP and walk through the Python code for some of the important steps in an NLP project.\n",
    "\n",
    "#### Author: Dr. Nimrita Koul, Associate Professor, Machine Learning\n",
    "#### Who this talk is suitable for: Data analysts, students and anyone interested in using Natural Language Processing\n",
    "#### Prerequisites: Python Programming, basic familiarity with concepts in data science and machine learning\n",
    "#### Duration: 4 Sessions of 1 hour each\n",
    "#### Time:  Starting 9th March 2023, Every Thursday,  8.00 pm to 9.00 pm Indian Standard Time\n",
    "#### Github URLs: \n",
    "\n",
    "Session1: \n",
    "https://github.com/NimritaKoul/NLP_WWC2023/blob/main/Session1_Introduction%20to%20Natural%20Language%20Processing.ipynb\n",
    "\n",
    "Session 2:\n",
    "https://github.com/NimritaKoul/NLP_WWC2023/blob/main/Session2_TextPreprocessing_Representation.ipynb\n",
    "\n",
    "Session 3:\n",
    "https://github.com/NimritaKoul/NLP_WWC2023/blob/main/Session3-TextClassification_Transformers.ipynb\n",
    "\n",
    "### Contents: \n",
    "\n",
    "#### Session 1:\n",
    "1. Introduction to Natural Language Processing, components, task, applications. \n",
    "2. Python Tools and Libraries for NLP - with focus on NLTK and SpaCy \n",
    "\n",
    "#### Session 2:\n",
    "1. Text Preprocessing - Cleaning, normalization - tokenization, stopword removal, case conversion, stemming, lemmatization, POS tagging, Named Entity Recognition\n",
    "2. Text Representation - Bag of word models, TF-IDF, Word Embeddings\n",
    "\n",
    "#### Session 3:\n",
    "<span style=\"color:Blue\"> \n",
    "1. Text Classification - Text classification models Naive Bayes, Decision Tree, RNN<br>\n",
    "2. Transformers\n",
    "</span>\n",
    "\n",
    "#### Session 4 :  Case Study\n",
    "1. Sentiment Analysis\n",
    "2. Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ea4a1",
   "metadata": {},
   "source": [
    "### Recap of Session 1 and 2:\n",
    "Session 1: Introduction to NLP, NLP tasks and applications. Introduction to Python libraries with focus on NLTK and SpaCy.\n",
    "\n",
    "Session 2: \n",
    "- Text Preprocessing techniques - lower casing, removing stopwords and punctuations, unicode normalization, removing special symbols like hashtags and mentions, tokenization, lemmatization, stemming.\n",
    "- Text Representation techniques - One hot encoding, BoW and BoW-N-gram model, TfIdf, Word2Vec, GloVe, FastText.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913dd2c",
   "metadata": {},
   "source": [
    "## Session 3: 23-March-2023\n",
    "### On agenda for today's session\n",
    "1. Text classification \n",
    "2. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34923371",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://developers.google.com/machine-learning/guides/text-classification\n",
    "2. https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "3. https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "4. https://mylearningsinaiml.wordpress.com/nlp/text-classification-using-nltk/\n",
    "5. https://realpython.com/python-keras-text-classification/\n",
    "6. https://goodboychan.github.io/python/machine_learning/natural_language_processing/2020/10/23/01-Text-Classification-with-NLTK.html\n",
    "7. https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "8. https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e\n",
    "9. https://realpython.com/python-keras-text-classification/\n",
    "10. https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
    "11. http://jalammar.github.io/illustrated-transformer/\n",
    "12. https://arxiv.org/abs/1706.03762\n",
    "13. https://daleonai.com/transformers-explained\n",
    "14. https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/\n",
    "15. https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "16. https://blog.paperspace.com/transformers-text-classification/\n",
    "17. https://arxiv.org/pdf/1409.0473.pdf\n",
    "18. https://www.dominodatalab.com/blog/transformers-self-attention-to-the-rescue#:~:text=What%20are%20Transformers%20Models%3F,sequences%20in%20deep%20learning%20applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e74a0",
   "metadata": {},
   "source": [
    "## What is text classification?\n",
    "\n",
    "- Text classification means to classify a piece of text (e.g.,a customer review, an email, a web page, a news article into some predefined categories or classes). \n",
    "- E.g., A positive, negative or neutral review comment, a spam or non-spam email, a web page as personal or business page, a news article about politics, sports or finance).\n",
    "\n",
    "<img src = 'newsclassification.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src = 'TextClassificationExample.png'>\n",
    "<center>Source: https://developers.google.com/machine-learning/guides/text-classification</center>\n",
    "\n",
    "\n",
    "- i.e., Text Classification is the task of assigning a label or class to a given text. For example, classifying an email as spam or not.\n",
    "\n",
    "- For the purpose of classification, often some highly informative features are identified from the input text. For example, a phishing email may have one or more of following features:\n",
    "\n",
    "<img src = 'Fake-emails.png' width = 700 height = 600>\n",
    "<center> Source: https://www.quostar.com/blog/business-scam-email-examples/</center>\n",
    "\n",
    "\n",
    "### Supervised Machine Learning \n",
    "\n",
    "- Supervised Machine Learning as well as unsupervised learning can be used for text classification in NLP.\n",
    "\n",
    "- Supervised learning involves training a classifier on a labelled dataset. Unsupervised learning does not require a labelled dataset, it used inherent properties of the data (similarity) for clustering data into groups.\n",
    "\n",
    "- Good quality labeled data, often from human annotators is very important for supervised machine learning.\n",
    "\n",
    "- The labelled data is often split into 3 portions, training set, validation set and test set. \n",
    "\n",
    "- The performance of a classifier is evaluated using metrics like - accuracy, precision, recall and F1 Score. \n",
    "\n",
    "#### General Steps in Text Classification\n",
    "\n",
    "<img src = 'Textclassificationflow.jpg'>\n",
    "<center> Source:https://developers.google.com/machine-learning/guides/text-classification/step-2-5#figure-5 </center>\n",
    "\n",
    "### Important use cases of text classification:\n",
    "\n",
    "1. Sentiment Analysis\n",
    "2. POS Tagging\n",
    "3. Natural Language Inference - To infer a relationship between two pieces of text - a premise and a hypothesis. Types of this relationship - entailment, contradiction and neutral. \n",
    "- entailment: hypothesis is supported by the premise\n",
    "- contradiction: hypothesis is negated by the premise\n",
    "- neutral: no relation between the hypothesis and the premise\n",
    "\n",
    "For example, \n",
    "\n",
    "    - Premise:     I am watching a movie right now. \n",
    "    - Hypothesis:  I am playing cricket right now.\n",
    "      Relationship Label: Contradiction\n",
    "\n",
    "\n",
    "    - Premise:    I am watching a movie right now.\n",
    "    - Hypothesis: Some movie is playing in front of me. \n",
    "      Relationship Label: Entailment\n",
    "    \n",
    "4. Checking Grammar Correctness: Aacceptable/not-acceptable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3fd15",
   "metadata": {},
   "source": [
    "## 1. Text classficiation with NLTK\n",
    " - Before running the code in todays session, you should have your installed Anaconda Python, NLTK, SpaCy, TensorFlow, Keras on your machine. \n",
    " - For the first example of text classification, We will use the <b>movie_reviews</b> corpus built into <b>nltk</b> library. \n",
    " - You can download the movie_reviews package using the nltk.download function:\n",
    "<code>\n",
    "import nltk\n",
    "nltk.download(\"movie_reviews\")\n",
    "</code>\n",
    "\n",
    "- The fileids() method allows us access the list of all files in a dataset in nltk.corpus. \n",
    "- The movie_reviews dataset has 2000 text files, each file has a fileid. Each of these files contains a review of a movie. 1000 of these files contain negative reviews, 1000 contain a positive review. The negative files are in a folder called 'neg',and all files containing positive reviews are in a folder called 'pos'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089ae924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required imports\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cc1d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total no. of review files in corpus\n",
    "##There are 1000 negative reviews, and 1000 positive reviews (one review per file)\n",
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3144590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first five filenames\n",
    "movie_reviews.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820d9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating filenames in two lists one for positive reviews, one for negative reviews(based on which folder they exists in corpus)\n",
    "negative_fileids = movie_reviews.fileids('neg')\n",
    "positive_fileids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840decb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are 1000 negative reviews, and 1000 positive reviews (one review per file)\n",
    "len(negative_fileids), len(positive_fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7255b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos/cv000_29590.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of first file in positive reviews folder\n",
    "positive_fileids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255508a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the text of review in above fileid using the raw method of the corpus.\n",
    "#Raw method gives the text of file\n",
    "print(movie_reviews.raw(fileids=positive_fileids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a01a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n"
     ]
    }
   ],
   "source": [
    "# printing the separate words/tokens of the above file, words() method of corpus gives the words\n",
    "print(movie_reviews.words(fileids=positive_fileids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1769c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will load all reviews and their labels (i.e., folder name pos or neg in which the review file is present)\n",
    "\n",
    "reviewswithcategory = [(list(movie_reviews.words(fileid)), category) \n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(reviewswithcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "12b41150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing - lower casing, removing stop words and punctuation marks\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def text_preprocessing(review):\n",
    "    review = [w.lower() for w in review]\n",
    "    review = [w.translate(str.maketrans('', '', string.punctuation)) for w in review]\n",
    "    review = [w for w in review if w not in stop_words]\n",
    "    review = list(filter(None, review)) #Remove empty strings\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "106a1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviewswithcategory = []\n",
    "for review, cat in reviewswithcategory:\n",
    "    cleanreview = text_preprocessing(review)\n",
    "    cleaned_reviewswithcategory.append((cleanreview, cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ac55ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our variable cleaned_reviewswithcategory is a list of tuples, \n",
    "# each tuple in it has a list of words and category label\n",
    "\n",
    "# here we all getting all words from all tuples into a single iterable\n",
    "allcleanwords = list(itertools.chain(*cleaned_reviewswithcategory))\n",
    "allcleanwordslist = []\n",
    "for m in range(len(allcleanwords)):\n",
    "   # traversing the inner lists\n",
    "   for n in range (len(allcleanwords[m])):\n",
    "      # Add each element to the result list\n",
    "      allcleanwordslist.append(allcleanwords[m][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d1801e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 9519), ('one', 5853), ('movie', 5774), ('like', 3690), ('even', 2565), ('time', 2411), ('good', 2411), ('story', 2170), ('would', 2110), ('much', 2050)]\n"
     ]
    }
   ],
   "source": [
    "# Using NLTK FreqDist for computing word frequencies\n",
    "freqd = nltk.FreqDist(allcleanwordslist)\n",
    "print(freqd.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2c1b967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying 5000 most frequent words from Frequency Distribution\n",
    "frequent_words = list(freqd.keys())[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a0346cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the presence of these most frequent words in our positive and negative reviews.\n",
    "# This function returns word and True if word is present, else it returns word and False. \n",
    "\n",
    "def extract_frequentwordfeatures(text):\n",
    "    words = set(text) # computing all unique words (vocabulary) in input text\n",
    "    features = {}\n",
    "    for w in frequent_words:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6818c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can do this for all of our documents, \n",
    "# saving the feature existence booleans and their respective positive or negative categories by doing:\n",
    "review_features = [(extract_frequentwordfeatures(review), category) for (review, category) in cleaned_reviewswithcategory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9a9b2d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'movie': True,\n",
       "   'concepts': True,\n",
       "   'often': True,\n",
       "   'pitched': True,\n",
       "   'producers': True,\n",
       "   'mathematical': True,\n",
       "   'formulas': True,\n",
       "   'involving': True,\n",
       "   'successful': True,\n",
       "   'films': True,\n",
       "   'past': True,\n",
       "   'undoubtedly': True,\n",
       "   'one': True,\n",
       "   'day': True,\n",
       "   'someone': True,\n",
       "   'said': True,\n",
       "   'evolution': True,\n",
       "   'ghostbusters': True,\n",
       "   'plus': True,\n",
       "   'men': True,\n",
       "   'black': True,\n",
       "   'tremors': True,\n",
       "   'sum': True,\n",
       "   'total': True,\n",
       "   'none': True,\n",
       "   'alienbusting': True,\n",
       "   'begins': True,\n",
       "   'meteor': True,\n",
       "   'lands': True,\n",
       "   'glen': True,\n",
       "   'canyon': True,\n",
       "   'arizona': True,\n",
       "   'community': True,\n",
       "   'college': True,\n",
       "   'science': True,\n",
       "   'profs': True,\n",
       "   'ira': True,\n",
       "   'kane': True,\n",
       "   'david': True,\n",
       "   'duchovny': True,\n",
       "   'harry': True,\n",
       "   'block': True,\n",
       "   'orlando': True,\n",
       "   'jones': True,\n",
       "   'take': True,\n",
       "   'samples': True,\n",
       "   'discover': True,\n",
       "   'celled': True,\n",
       "   'organisms': True,\n",
       "   'inside': True,\n",
       "   'evolving': True,\n",
       "   'rapidly': True,\n",
       "   'hours': True,\n",
       "   'took': True,\n",
       "   'millions': True,\n",
       "   'years': True,\n",
       "   'life': True,\n",
       "   'earth': True,\n",
       "   'time': True,\n",
       "   'wondering': True,\n",
       "   'hey': True,\n",
       "   'government': True,\n",
       "   'usually': True,\n",
       "   'come': True,\n",
       "   'whole': True,\n",
       "   'area': True,\n",
       "   'movies': True,\n",
       "   'came': True,\n",
       "   'leader': True,\n",
       "   'pack': True,\n",
       "   'gen': True,\n",
       "   'woodman': True,\n",
       "   'ted': True,\n",
       "   'levine': True,\n",
       "   'turns': True,\n",
       "   'old': True,\n",
       "   'nemesis': True,\n",
       "   'blocked': True,\n",
       "   'research': True,\n",
       "   'meanwhile': True,\n",
       "   'continue': True,\n",
       "   'grow': True,\n",
       "   'large': True,\n",
       "   'enough': True,\n",
       "   'start': True,\n",
       "   'attacking': True,\n",
       "   'people': True,\n",
       "   'gained': True,\n",
       "   'friend': True,\n",
       "   'dr': True,\n",
       "   'allison': True,\n",
       "   'reed': True,\n",
       "   'julianne': True,\n",
       "   'moore': True,\n",
       "   'center': True,\n",
       "   'disease': True,\n",
       "   'control': True,\n",
       "   'country': True,\n",
       "   'club': True,\n",
       "   'poolboy': True,\n",
       "   'wayne': True,\n",
       "   'green': True,\n",
       "   'seann': True,\n",
       "   'william': True,\n",
       "   'scott': True,\n",
       "   'also': True,\n",
       "   'attached': True,\n",
       "   'merry': True,\n",
       "   'band': True,\n",
       "   'find': True,\n",
       "   'means': True,\n",
       "   'stop': True,\n",
       "   'aliens': True,\n",
       "   'director': True,\n",
       "   'ivan': True,\n",
       "   'reitman': True,\n",
       "   'seems': True,\n",
       "   'revisiting': True,\n",
       "   'biggest': True,\n",
       "   'hit': True,\n",
       "   '1984': True,\n",
       "   'falls': True,\n",
       "   'miserably': True,\n",
       "   'short': True,\n",
       "   'reason': True,\n",
       "   'readily': True,\n",
       "   'apparent': True,\n",
       "   'three': True,\n",
       "   'really': True,\n",
       "   'funny': True,\n",
       "   'guys': True,\n",
       "   'scientists': True,\n",
       "   'bill': True,\n",
       "   'murray': True,\n",
       "   'dan': True,\n",
       "   'aykroyd': True,\n",
       "   'harold': True,\n",
       "   'ramis': True,\n",
       "   'guy': True,\n",
       "   'adlib': True,\n",
       "   'milk': True,\n",
       "   'comic': True,\n",
       "   'potential': True,\n",
       "   'lines': True,\n",
       "   'prove': True,\n",
       "   'useless': True,\n",
       "   'anyone': True,\n",
       "   'else': True,\n",
       "   'hands': True,\n",
       "   'good': True,\n",
       "   'measure': True,\n",
       "   'talents': True,\n",
       "   'rick': True,\n",
       "   'moranis': True,\n",
       "   'offers': True,\n",
       "   'much': True,\n",
       "   'funnier': True,\n",
       "   'american': True,\n",
       "   'pie': True,\n",
       "   'road': True,\n",
       "   'trip': True,\n",
       "   'amusing': True,\n",
       "   'scene': True,\n",
       "   'sings': True,\n",
       "   'beautiful': True,\n",
       "   'dragon': True,\n",
       "   'like': True,\n",
       "   'alien': True,\n",
       "   'draw': True,\n",
       "   'trap': True,\n",
       "   'unable': True,\n",
       "   'make': True,\n",
       "   'thin': True,\n",
       "   'material': True,\n",
       "   'writers': True,\n",
       "   'gave': True,\n",
       "   'actor': True,\n",
       "   'search': True,\n",
       "   'forte': True,\n",
       "   'looked': True,\n",
       "   'gritty': True,\n",
       "   'action': True,\n",
       "   'hero': True,\n",
       "   'side': True,\n",
       "   'playing': True,\n",
       "   'god': True,\n",
       "   'romantic': True,\n",
       "   'return': True,\n",
       "   'wants': True,\n",
       "   'wacky': True,\n",
       "   'comedic': True,\n",
       "   'talent': True,\n",
       "   'look': True,\n",
       "   'back': True,\n",
       "   'tv': True,\n",
       "   'five': True,\n",
       "   'another': True,\n",
       "   'sad': True,\n",
       "   'case': True,\n",
       "   'needs': True,\n",
       "   'selective': True,\n",
       "   'accepts': True,\n",
       "   'talented': True,\n",
       "   'actress': True,\n",
       "   'impressive': True,\n",
       "   'list': True,\n",
       "   'credits': True,\n",
       "   'award': True,\n",
       "   'nominations': True,\n",
       "   'trying': True,\n",
       "   'slapstick': True,\n",
       "   'comedy': True,\n",
       "   'character': True,\n",
       "   'tendency': True,\n",
       "   'bump': True,\n",
       "   'things': True,\n",
       "   'trait': True,\n",
       "   'comes': True,\n",
       "   'pathetic': True,\n",
       "   'plea': True,\n",
       "   'laughs': True,\n",
       "   'sole': True,\n",
       "   'saving': True,\n",
       "   'grace': True,\n",
       "   'film': True,\n",
       "   'funniest': True,\n",
       "   'bug': True,\n",
       "   'invades': True,\n",
       "   'body': True,\n",
       "   'pulled': True,\n",
       "   'ass': True,\n",
       "   'cast': True,\n",
       "   'member': True,\n",
       "   'ball': True,\n",
       "   'run': True,\n",
       "   'carry': True,\n",
       "   'found': True,\n",
       "   'similar': True,\n",
       "   'situation': True,\n",
       "   'replacements': True,\n",
       "   'ensemble': True,\n",
       "   'eventually': True,\n",
       "   'project': True,\n",
       "   'major': True,\n",
       "   'star': True,\n",
       "   'n': False,\n",
       "   'e': False,\n",
       "   'g': False,\n",
       "   'quilt': False,\n",
       "   'fabric': False,\n",
       "   'thread': False,\n",
       "   'patchwork': False,\n",
       "   'design': False,\n",
       "   'unique': False,\n",
       "   'story': False,\n",
       "   'stories': False,\n",
       "   'tell': False,\n",
       "   'first': False,\n",
       "   'hollywood': False,\n",
       "   'release': False,\n",
       "   'australian': False,\n",
       "   'jocelyn': False,\n",
       "   'moorhouse': False,\n",
       "   'proof': False,\n",
       "   'understand': False,\n",
       "   'tales': False,\n",
       "   'seven': False,\n",
       "   'makers': False,\n",
       "   'imbue': False,\n",
       "   'creation': False,\n",
       "   'passion': False,\n",
       "   'vitality': False,\n",
       "   'sorrows': False,\n",
       "   'joys': False,\n",
       "   'longings': False,\n",
       "   'sufferings': False,\n",
       "   'loves': False,\n",
       "   'women': False,\n",
       "   'sewn': False,\n",
       "   'bittersweet': False,\n",
       "   'memories': False,\n",
       "   'given': False,\n",
       "   'expression': False,\n",
       "   'young': False,\n",
       "   'woman': False,\n",
       "   'applies': False,\n",
       "   'lessons': False,\n",
       "   'uncertain': False,\n",
       "   'future': False,\n",
       "   'motion': False,\n",
       "   'pictures': False,\n",
       "   'filled': False,\n",
       "   'male': False,\n",
       "   'bonding': False,\n",
       "   'rituals': False,\n",
       "   'presents': False,\n",
       "   'distinctly': False,\n",
       "   'feminine': False,\n",
       "   'alternative': False,\n",
       "   'quilting': False,\n",
       "   'bee': False,\n",
       "   'consists': False,\n",
       "   'members': False,\n",
       "   'sisters': False,\n",
       "   'gladys': False,\n",
       "   'ann': False,\n",
       "   'bancroft': False,\n",
       "   'hy': False,\n",
       "   'ellen': False,\n",
       "   'burstyn': False,\n",
       "   'sophia': False,\n",
       "   'lois': False,\n",
       "   'smith': False,\n",
       "   'known': False,\n",
       "   'frightening': False,\n",
       "   'children': False,\n",
       "   'emma': False,\n",
       "   'jean': False,\n",
       "   'simmons': False,\n",
       "   'timid': False,\n",
       "   'wife': False,\n",
       "   'perpetually': False,\n",
       "   'unfaithful': False,\n",
       "   'man': False,\n",
       "   'constance': False,\n",
       "   'kate': False,\n",
       "   'nelligan': False,\n",
       "   'affair': False,\n",
       "   'husband': False,\n",
       "   'anna': False,\n",
       "   'maya': False,\n",
       "   'angelou': False,\n",
       "   'group': False,\n",
       "   'marianna': False,\n",
       "   'alfre': False,\n",
       "   'woodard': False,\n",
       "   'daughter': False,\n",
       "   'busy': False,\n",
       "   'wedding': False,\n",
       "   'granddaughter': False,\n",
       "   'finn': False,\n",
       "   'winona': False,\n",
       "   'ryder': False,\n",
       "   'become': False,\n",
       "   'engaged': False,\n",
       "   'spending': False,\n",
       "   'summer': False,\n",
       "   'away': False,\n",
       "   'fiance': False,\n",
       "   'decide': False,\n",
       "   'whether': False,\n",
       "   'lifelong': False,\n",
       "   'commitment': False,\n",
       "   'better': False,\n",
       "   'marry': False,\n",
       "   'lover': False,\n",
       "   'greatest': False,\n",
       "   'pleasures': False,\n",
       "   'watching': False,\n",
       "   'array': False,\n",
       "   'fine': False,\n",
       "   'performances': False,\n",
       "   'minor': False,\n",
       "   'players': False,\n",
       "   'rip': False,\n",
       "   'torn': False,\n",
       "   'claire': False,\n",
       "   'danes': False,\n",
       "   'capshaw': False,\n",
       "   'overflowing': False,\n",
       "   'realized': False,\n",
       "   'fact': False,\n",
       "   'cover': False,\n",
       "   'many': False,\n",
       "   'script': False,\n",
       "   'weaknesses': False,\n",
       "   'basically': False,\n",
       "   'unremarkable': False,\n",
       "   'affecting': False,\n",
       "   'tale': False,\n",
       "   'love': False,\n",
       "   'across': False,\n",
       "   'generations': False,\n",
       "   'sort': False,\n",
       "   'truncated': False,\n",
       "   'version': False,\n",
       "   'joy': False,\n",
       "   'luck': False,\n",
       "   'dash': False,\n",
       "   'fried': False,\n",
       "   'tomatoes': False,\n",
       "   'added': False,\n",
       "   'probably': False,\n",
       "   'characters': False,\n",
       "   'instead': False,\n",
       "   'getting': False,\n",
       "   'know': False,\n",
       "   'presented': False,\n",
       "   'quick': False,\n",
       "   'glimpses': False,\n",
       "   'single': False,\n",
       "   'defining': False,\n",
       "   'event': False,\n",
       "   'lives': False,\n",
       "   'learn': False,\n",
       "   'root': False,\n",
       "   'smoldering': False,\n",
       "   'resentment': False,\n",
       "   'told': False,\n",
       "   'reasons': False,\n",
       "   'stays': False,\n",
       "   'irascible': False,\n",
       "   'episodes': False,\n",
       "   'well': False,\n",
       "   'soul': False,\n",
       "   'mate': False,\n",
       "   'entered': False,\n",
       "   'meant': False,\n",
       "   'provide': False,\n",
       "   'framework': False,\n",
       "   'go': False,\n",
       "   'forward': False,\n",
       "   'marriage': False,\n",
       "   'dally': False,\n",
       "   'hunky': False,\n",
       "   'stranger': False,\n",
       "   'emotional': False,\n",
       "   'epiphany': False,\n",
       "   'difficult': False,\n",
       "   'connect': False,\n",
       "   'moments': False,\n",
       "   'pass': False,\n",
       "   'quickly': False,\n",
       "   'certainly': False,\n",
       "   'feel': False,\n",
       "   'something': False,\n",
       "   'eight': False,\n",
       "   'principals': False,\n",
       "   'investment': False,\n",
       "   'tenuous': False,\n",
       "   'depth': False,\n",
       "   'pull': False,\n",
       "   'viewer': False,\n",
       "   'way': False,\n",
       "   'personalities': False,\n",
       "   'relationships': False,\n",
       "   'half': False,\n",
       "   'formed': False,\n",
       "   'patches': False,\n",
       "   'missing': False,\n",
       "   'put': False,\n",
       "   'bluntly': False,\n",
       "   'enjoyed': False,\n",
       "   'nicely': False,\n",
       "   'understated': False,\n",
       "   'drama': False,\n",
       "   'lot': False,\n",
       "   'say': False,\n",
       "   'monogamy': False,\n",
       "   'segment': False,\n",
       "   'far': False,\n",
       "   'compelling': False,\n",
       "   'focal': False,\n",
       "   'point': False,\n",
       "   'actions': False,\n",
       "   'shaped': False,\n",
       "   'everyone': False,\n",
       "   'experiences': False,\n",
       "   'varying': False,\n",
       "   'degrees': False,\n",
       "   'disappointment': False,\n",
       "   'resonance': False,\n",
       "   'privilege': False,\n",
       "   'seeing': False,\n",
       "   'top': False,\n",
       "   'form': False,\n",
       "   'allows': False,\n",
       "   'enjoy': False,\n",
       "   'picture': False,\n",
       "   'even': False,\n",
       "   'somewhat': False,\n",
       "   'conventional': False,\n",
       "   'p': False,\n",
       "   'o': False,\n",
       "   's': False,\n",
       "   'stands': False,\n",
       "   'moment': False,\n",
       "   'soon': False,\n",
       "   'forget': False,\n",
       "   'giant': False,\n",
       "   'ogre': False,\n",
       "   'flips': False,\n",
       "   'pages': False,\n",
       "   'cliche': False,\n",
       "   'fairy': False,\n",
       "   'narrating': False,\n",
       "   'every': False,\n",
       "   'bit': False,\n",
       "   'dull': False,\n",
       "   'inspiration': False,\n",
       "   'holds': False,\n",
       "   'leads': False,\n",
       "   'believe': False,\n",
       "   'serves': False,\n",
       "   'prologue': False,\n",
       "   'shrek': False,\n",
       "   'dreamworks': False,\n",
       "   'second': False,\n",
       "   'computer': False,\n",
       "   'animated': False,\n",
       "   'feature': False,\n",
       "   'pricelessly': False,\n",
       "   'hilarious': False,\n",
       "   'cinema': False,\n",
       "   'page': False,\n",
       "   'toilet': False,\n",
       "   'paper': False,\n",
       "   'opening': False,\n",
       "   'infer': False,\n",
       "   'defying': False,\n",
       "   'expectations': False,\n",
       "   'regarding': False,\n",
       "   'standard': False,\n",
       "   'disney': False,\n",
       "   'esque': False,\n",
       "   'although': False,\n",
       "   'rampant': False,\n",
       "   'hilarity': False,\n",
       "   'dot': False,\n",
       "   'true': False,\n",
       "   'charm': False,\n",
       "   'lies': False,\n",
       "   'bold': False,\n",
       "   'elements': False,\n",
       "   'friendship': False,\n",
       "   'courage': False,\n",
       "   'acceptance': False,\n",
       "   'excelled': False,\n",
       "   'outstanding': False,\n",
       "   'direction': False,\n",
       "   'stunning': False,\n",
       "   'voice': False,\n",
       "   'work': False,\n",
       "   'importantly': False,\n",
       "   'witty': False,\n",
       "   'screenplay': False,\n",
       "   'going': False,\n",
       "   'meets': False,\n",
       "   'eye': False,\n",
       "   'features': False,\n",
       "   'abundance': False,\n",
       "   'humor': False,\n",
       "   'related': False,\n",
       "   'directly': False,\n",
       "   'toward': False,\n",
       "   'adults': False,\n",
       "   'positive': False,\n",
       "   'values': False,\n",
       "   'aimed': False,\n",
       "   'younger': False,\n",
       "   'fulfill': False,\n",
       "   'storyline': False,\n",
       "   'previously': False,\n",
       "   'mentioned': False,\n",
       "   'position': False,\n",
       "   'absolute': False,\n",
       "   'nothingness': False,\n",
       "   'within': False,\n",
       "   'jolly': False,\n",
       "   'fellow': False,\n",
       "   'outsider': False,\n",
       "   'donkey': False,\n",
       "   'species': False,\n",
       "   'talks': False,\n",
       "   'remains': False,\n",
       "   'reluctant': False,\n",
       "   'warm': False,\n",
       "   'portion': False,\n",
       "   'mutual': False,\n",
       "   'understanding': False,\n",
       "   'acknowledging': False,\n",
       "   'common': False,\n",
       "   'state': False,\n",
       "   'outcasts': False,\n",
       "   'world': False,\n",
       "   'pair': False,\n",
       "   'team': False,\n",
       "   'retrieve': False,\n",
       "   'princess': False,\n",
       "   'fiona': False,\n",
       "   'castle': False,\n",
       "   'guarded': False,\n",
       "   'powerful': False,\n",
       "   'lord': False,\n",
       "   'farquaad': False,\n",
       "   'learning': False,\n",
       "   'quite': False,\n",
       "   'along': False,\n",
       "   'exhibits': False,\n",
       "   'core': False,\n",
       "   'theme': False,\n",
       "   'need': False,\n",
       "   'lean': False,\n",
       "   'exemplifying': False,\n",
       "   'child': False,\n",
       "   'may': False,\n",
       "   'witness': False,\n",
       "   'acting': False,\n",
       "   'honest': False,\n",
       "   'companion': False,\n",
       "   'unquestionably': False,\n",
       "   'affect': False,\n",
       "   'endure': False,\n",
       "   'supposed': False,\n",
       "   'freaks': False,\n",
       "   'nature': False,\n",
       "   'due': False,\n",
       "   'differences': False,\n",
       "   'others': False,\n",
       "   'cling': False,\n",
       "   'support': False,\n",
       "   'guidance': False,\n",
       "   'journey': False,\n",
       "   'demonstrates': False,\n",
       "   'two': False,\n",
       "   'storylines': False,\n",
       "   'rescue': False,\n",
       "   'immeasurable': False,\n",
       "   'cowardice': False,\n",
       "   'reveals': False,\n",
       "   'encounters': False,\n",
       "   'walking': False,\n",
       "   'shaky': False,\n",
       "   'bridge': False,\n",
       "   'living': False,\n",
       "   'alone': False,\n",
       "   'withholds': False,\n",
       "   'fears': False,\n",
       "   'continuously': False,\n",
       "   'puts': False,\n",
       "   'aside': False,\n",
       "   'without': False,\n",
       "   'self': False,\n",
       "   'acknowledgment': False,\n",
       "   'yapping': False,\n",
       "   'must': False,\n",
       "   'face': False,\n",
       "   'phobias': False,\n",
       "   'fire': False,\n",
       "   'breathing': False,\n",
       "   'guarding': False,\n",
       "   'either': False,\n",
       "   'dies': False,\n",
       "   'terror': False,\n",
       "   'triumphs': False,\n",
       "   'wisely': False,\n",
       "   'chooses': False,\n",
       "   'pretending': False,\n",
       "   'dangerous': False,\n",
       "   'actually': False,\n",
       "   'faces': False,\n",
       "   'allowing': False,\n",
       "   'overcome': False,\n",
       "   'horror': False,\n",
       "   'likely': False,\n",
       "   'continued': False,\n",
       "   'insistence': False,\n",
       "   'secondly': False,\n",
       "   'upon': False,\n",
       "   'redeeming': False,\n",
       "   'finds': False,\n",
       "   'plans': False,\n",
       "   'whomever': False,\n",
       "   'saves': False,\n",
       "   'satisfy': False,\n",
       "   'desire': False,\n",
       "   'human': False,\n",
       "   'knight': False,\n",
       "   'shining': False,\n",
       "   'armor': False,\n",
       "   'throughout': False,\n",
       "   'deep': False,\n",
       "   'perpetual': False,\n",
       "   'misgivings': False,\n",
       "   'inhering': False,\n",
       "   'struggles': False,\n",
       "   'determine': False,\n",
       "   'reveal': False,\n",
       "   'identity': False,\n",
       "   'ongoing': False,\n",
       "   'struggle': False,\n",
       "   'occurs': False,\n",
       "   'reluctance': False,\n",
       "   'truth': False,\n",
       "   'courageously': False,\n",
       "   'conquers': False,\n",
       "   'fear': False,\n",
       "   'telling': False,\n",
       "   'forbidden': False,\n",
       "   'secret': False,\n",
       "   'realize': False,\n",
       "   'complicated': False,\n",
       "   'makes': False,\n",
       "   'sets': False,\n",
       "   'mind': False,\n",
       "   'overcoming': False,\n",
       "   'particular': False,\n",
       "   'endless': False,\n",
       "   'possibilities': False,\n",
       "   'exist': False,\n",
       "   'friendships': False,\n",
       "   'prevail': False,\n",
       "   'central': False,\n",
       "   'heart': False,\n",
       "   'stretch': False,\n",
       "   'development': False,\n",
       "   'surprisingly': False,\n",
       "   'dark': False,\n",
       "   'spell': False,\n",
       "   'ago': False,\n",
       "   'transforms': False,\n",
       "   'night': False,\n",
       "   'hold': False,\n",
       "   'beauty': False,\n",
       "   'truly': False,\n",
       "   'simply': False,\n",
       "   'act': False,\n",
       "   'pursue': False,\n",
       "   'relationship': False,\n",
       "   'knowing': False,\n",
       "   'platform': False,\n",
       "   'gives': False,\n",
       "   'inner': False,\n",
       "   'demons': False,\n",
       "   'ridicule': False,\n",
       "   'hatred': False,\n",
       "   'transformation': False,\n",
       "   'meaning': False,\n",
       "   'technical': False,\n",
       "   'directors': False,\n",
       "   'adam': False,\n",
       "   'adamson': False,\n",
       "   'vicky': False,\n",
       "   'jenson': False,\n",
       "   'pace': False,\n",
       "   'consistent': False,\n",
       "   'satisfying': False,\n",
       "   'manner': False,\n",
       "   'evenly': False,\n",
       "   'distributing': False,\n",
       "   'learned': False,\n",
       "   'spring': False,\n",
       "   'thanks': False,\n",
       "   'brilliant': False,\n",
       "   'mike': False,\n",
       "   'myers': False,\n",
       "   'eddie': False,\n",
       "   'murphy': False,\n",
       "   'cameron': False,\n",
       "   'diaz': False,\n",
       "   'creates': False,\n",
       "   'lovable': False,\n",
       "   'troubled': False,\n",
       "   'whose': False,\n",
       "   'color': False,\n",
       "   'skin': False,\n",
       "   'perhaps': False,\n",
       "   'envy': False,\n",
       "   'normal': False,\n",
       "   'beings': False,\n",
       "   'succeeds': False,\n",
       "   'recreating': False,\n",
       "   'high': False,\n",
       "   'strung': False,\n",
       "   'persona': False,\n",
       "   'screen': False,\n",
       "   'exudes': False,\n",
       "   'charisma': False,\n",
       "   'innocence': False,\n",
       "   'giving': False,\n",
       "   'sense': False,\n",
       "   'power': False,\n",
       "   'female': False,\n",
       "   'lack': False,\n",
       "   'elevate': False,\n",
       "   'ultimate': False,\n",
       "   'factor': False,\n",
       "   'succeeding': False,\n",
       "   'written': False,\n",
       "   'elliott': False,\n",
       "   'terry': False,\n",
       "   'rossio': False,\n",
       "   'joe': False,\n",
       "   'stillman': False,\n",
       "   'roger': False,\n",
       "   'h': False,\n",
       "   'schulman': False,\n",
       "   'never': False,\n",
       "   'underestimates': False,\n",
       "   'intelligence': False,\n",
       "   'combining': False,\n",
       "   'spoken': False,\n",
       "   'alike': False,\n",
       "   'capping': False,\n",
       "   'constant': False,\n",
       "   'tongue': False,\n",
       "   'cheek': False,\n",
       "   'pokes': False,\n",
       "   'fun': False,\n",
       "   'classic': False,\n",
       "   'fresh': False,\n",
       "   'entertaining': False,\n",
       "   'delivers': False,\n",
       "   'executive': False,\n",
       "   'jeffrey': False,\n",
       "   'katzenberg': False,\n",
       "   'upped': False,\n",
       "   'creating': False,\n",
       "   'instant': False,\n",
       "   'live': False,\n",
       "   'ages': False,\n",
       "   'smaller': False,\n",
       "   'laugh': False,\n",
       "   'cheer': False,\n",
       "   'harder': False,\n",
       "   'leave': False,\n",
       "   'breaks': False,\n",
       "   'stereotypes': False,\n",
       "   'definite': False,\n",
       "   'private': False,\n",
       "   'matter': False,\n",
       "   'based': False,\n",
       "   'sherri': False,\n",
       "   'finkbine': False,\n",
       "   'events': False,\n",
       "   '1960s': False,\n",
       "   'becoming': False,\n",
       "   'pregnant': False,\n",
       "   'fifth': False,\n",
       "   'learns': False,\n",
       "   'tranquilizers': False,\n",
       "   'taken': False,\n",
       "   'seriously': False,\n",
       "   'deformed': False,\n",
       "   'unborn': False,\n",
       "   'help': False,\n",
       "   'doctor': False,\n",
       "   'abortion': False,\n",
       "   'illegal': False,\n",
       "   'made': False,\n",
       "   'arrangements': False,\n",
       "   'procedure': False,\n",
       "   'performed': False,\n",
       "   'withing': False,\n",
       "   'law': False,\n",
       "   'claiming': False,\n",
       "   'endangering': False,\n",
       "   'mother': False,\n",
       "   'everything': False,\n",
       "   'uncontrollable': False,\n",
       "   'urge': False,\n",
       "   'talk': False,\n",
       "   'local': False,\n",
       "   'reporter': False,\n",
       "   'media': False,\n",
       "   'frenzy': False,\n",
       "   'immediately': False,\n",
       "   'hated': False,\n",
       "   'fired': False,\n",
       "   'jobs': False,\n",
       "   'reporters': False,\n",
       "   'surrounding': False,\n",
       "   'home': False,\n",
       "   'times': False,\n",
       "   'countries': False,\n",
       "   'perform': False,\n",
       "   'causes': False,\n",
       "   'emotion': False,\n",
       "   'problems': False,\n",
       "   'involved': False,\n",
       "   'fascinating': False,\n",
       "   'hard': False,\n",
       "   'happened': False,\n",
       "   '30': False,\n",
       "   'flaw': False,\n",
       "   'ever': False,\n",
       "   'satisfactory': False,\n",
       "   'confrontation': False,\n",
       "   'opponents': False,\n",
       "   'passing': False,\n",
       "   'street': False,\n",
       "   'telss': False,\n",
       "   'burn': False,\n",
       "   'hell': False,\n",
       "   'appropriately': False,\n",
       "   'yells': False,\n",
       "   'answer': False,\n",
       "   'apparently': False,\n",
       "   'wanted': False,\n",
       "   'choice': False,\n",
       "   'important': False,\n",
       "   'wanting': False,\n",
       "   'cause': False,\n",
       "   'severe': False,\n",
       "   'changes': False,\n",
       "   'family': False,\n",
       "   'could': False,\n",
       "   'beneficial': False,\n",
       "   'sydney': False,\n",
       "   'pollack': False,\n",
       "   'served': False,\n",
       "   'producer': False,\n",
       "   'hbo': False,\n",
       "   'austin': False,\n",
       "   'powers': False,\n",
       "   'spy': False,\n",
       "   'shagged': False,\n",
       "   'original': False,\n",
       "   'zany': False,\n",
       "   'silly': False,\n",
       "   'totally': False,\n",
       "   'enjoyable': False,\n",
       "   'predecessor': False,\n",
       "   'would': False,\n",
       "   'easily': False,\n",
       "   'exact': False,\n",
       "   'opposite': False,\n",
       "   'refreshing': False,\n",
       "   'audience': False,\n",
       "   'clever': False,\n",
       "   'parody': False,\n",
       "   'filmmakers': False,\n",
       "   'bombard': False,\n",
       "   'us': False,\n",
       "   'used': False,\n",
       "   'tired': False,\n",
       "   'jokes': False,\n",
       "   'played': False,\n",
       "   'fraction': False,\n",
       "   'giddy': False,\n",
       "   'enthusiasm': False,\n",
       "   'displayed': False,\n",
       "   'confronts': False,\n",
       "   'henchmen': False,\n",
       "   'cliff': False,\n",
       "   'pushed': False,\n",
       "   'ravine': False,\n",
       "   'bad': False,\n",
       "   'course': False,\n",
       "   'assumed': False,\n",
       "   'dead': False,\n",
       "   'suddenly': False,\n",
       "   'hear': False,\n",
       "   'pleading': False,\n",
       "   'coming': False,\n",
       "   'injured': False,\n",
       "   'international': False,\n",
       "   'mystery': False,\n",
       "   'insulted': False,\n",
       "   'overestimated': False,\n",
       "   'average': False,\n",
       "   'goer': False,\n",
       "   'considering': False,\n",
       "   '200': False,\n",
       "   'million': False,\n",
       "   'domestic': False,\n",
       "   'theatrical': False,\n",
       "   'grosses': False,\n",
       "   'sound': False,\n",
       "   'franchise': False,\n",
       "   'heading': False,\n",
       "   'though': False,\n",
       "   'still': False,\n",
       "   'cannot': False,\n",
       "   'deny': False,\n",
       "   'merits': False,\n",
       "   'basic': False,\n",
       "   'plot': False,\n",
       "   'delightfully': False,\n",
       "   'sustain': False,\n",
       "   'interest': False,\n",
       "   'evil': False,\n",
       "   'roles': False,\n",
       "   'constructed': False,\n",
       "   'machine': False,\n",
       "   'goes': False,\n",
       "   'year': False,\n",
       "   '1969': False,\n",
       "   'attempt': False,\n",
       "   'snatch': False,\n",
       "   'mojo': False,\n",
       "   'word': False,\n",
       "   'whilst': False,\n",
       "   'cryogenically': False,\n",
       "   'frozen': False,\n",
       "   'gets': False,\n",
       "   'wind': False,\n",
       "   'great': False,\n",
       "   'intro': False,\n",
       "   'explaining': False,\n",
       "   ...},\n",
       "  'neg')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature set of first review\n",
    "review_features[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5871b7",
   "metadata": {},
   "source": [
    "##### Now our dataset has reviews and a category label for each review.\n",
    "Each review is not just a list of words, but a list of word and a boolean True or False indicating whether the word is present in the review or not. There are a total of 2000 reviews in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f9598027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the documents into training and test portions\n",
    "train_data = review_features[:1800]\n",
    "# set that we'll test against.\n",
    "test_data = review_features[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "931fda30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Naive Bayes classifier from NLTK to train \n",
    "clf_nb = nltk.NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "98c22197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Easy enough, now it is trained. Next, we can test it:\n",
    "print(\" Accuracy:\",(nltk.classify.accuracy(clf_nb, test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "abe41d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            breathtaking = True              pos : neg    =     12.6 : 1.0\n",
      "              astounding = True              pos : neg    =     12.3 : 1.0\n",
      "                  finest = True              pos : neg    =     11.3 : 1.0\n",
      "                  hatred = True              pos : neg    =     10.3 : 1.0\n",
      "                seamless = True              pos : neg    =     10.3 : 1.0\n",
      "               insulting = True              neg : pos    =      9.8 : 1.0\n",
      "             outstanding = True              pos : neg    =      9.7 : 1.0\n",
      "                   vader = True              pos : neg    =      9.7 : 1.0\n",
      "               ludicrous = True              neg : pos    =      9.6 : 1.0\n",
      "                 conveys = True              pos : neg    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Displaying most relevant 10 words that determine classifier's labelling decision\n",
    "\n",
    "clf_nb.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aba174",
   "metadata": {},
   "source": [
    "## Example 2. Classifying with more classifiers from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7082c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Classifier Accuracy: 0.825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83       100\n",
      "         pos       0.83      0.83      0.83       100\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n",
      "Confusion Matrix: \n",
      " [[80 20]\n",
      " [15 85]]\n",
      "\n",
      "Multinomial Naive Bayes Classifier Accuracy: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83       100\n",
      "         pos       0.83      0.83      0.83       100\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n",
      "Confusion Matrix: \n",
      " [[84 16]\n",
      " [16 84]]\n",
      "\n",
      "Support Vector Machine Classifier Accuracy: 0.805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83       100\n",
      "         pos       0.83      0.83      0.83       100\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n",
      "Confusion Matrix: \n",
      " [[79 21]\n",
      " [18 82]]\n",
      "\n",
      "KNN Classifier Accuracy: 0.505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83       100\n",
      "         pos       0.83      0.83      0.83       100\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n",
      "Confusion Matrix: \n",
      " [[95  5]\n",
      " [94  6]]\n",
      "\n",
      "DT Classifier Accuracy: 0.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83       100\n",
      "         pos       0.83      0.83      0.83       100\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n",
      "Confusion Matrix: \n",
      " [[51 49]\n",
      " [27 73]]\n",
      "\n",
      "RF Classifier Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.83      0.83       100\n",
      "         pos       0.83      0.83      0.83       100\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n",
      "Confusion Matrix: \n",
      " [[76 24]\n",
      " [18 82]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "names = ['Logistic Regression','Multinomial Naive Bayes', 'Support Vector Machine','KNN', 'DT', 'RF']\n",
    "\n",
    "classifiers = [LogisticRegression(),MultinomialNB(),SVC(kernel='linear'),KNeighborsClassifier(), DecisionTreeClassifier(),RandomForestClassifier()]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "text_features, labels = zip(*test_data)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_clf = SklearnClassifier(model)\n",
    "    nltk_clf.train(train_data)\n",
    "    accuracy = nltk.classify.accuracy(nltk_clf, test_data)\n",
    "    print(\"\\n{} Classifier Accuracy: {}\".format(name, accuracy))\n",
    "    \n",
    "    y_pred = nltk_clf.classify_many(text_features)\n",
    "    print(classification_report(labels, prediction))\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(labels, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c17c3",
   "metadata": {},
   "source": [
    "### 3rd Example:  Sentiment Classificaion with Keras\n",
    "\n",
    "URl: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "Data Set Information: \n",
    "It contains sentences labelled with positive or negative sentiment. Sentiment is in the form of a score, score is 1 for positive sentiment, 0 for negative. In the dataset there are sentences from three different websites/fields: imdb.com, amazon.com, yelp.com. For each website, there exist 500 positive and 500 negative sentences. We will use only the sentiments from Amazon.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "f606399d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sentiment_labelled_sentences/amazon_cells_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "905c3c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      So there is no way for me to plug it in here i...\n",
       "1                            Good case, Excellent value.\n",
       "2                                 Great for the jawbone.\n",
       "3      Tied to charger for conversations lasting more...\n",
       "4                                      The mic is great.\n",
       "                             ...                        \n",
       "995    The screen does get smudged easily because it ...\n",
       "996    What a piece of junk.. I lose more calls on th...\n",
       "997                         Item Does Not Match Picture.\n",
       "998    The only thing that disappoint me is the infra...\n",
       "999    You can not answer calls with the unit, never ...\n",
       "Name: sentence, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6c1f1105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         way plug u unless go converter\n",
       "1                              good case excellent value\n",
       "2                                          great jawbone\n",
       "3      tied charger conversation lasting minutesmajor...\n",
       "4                                              mic great\n",
       "                             ...                        \n",
       "995             screen get smudged easily touch ear face\n",
       "996                           piece junk lose call phone\n",
       "997                                   item match picture\n",
       "998                 thing disappoint infra red port irda\n",
       "999                        answer call unit never worked\n",
       "Name: sentence, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This process_text() function returns list of cleaned tokens of the text\n",
    "import numpy\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "df['sentence'] = df['sentence'].apply(process_text)\n",
    "df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f8b798c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking cleaned text and sentiment labels in two separate variables\n",
    "sentences = df['sentence']\n",
    "labels = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "64116f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train-test portions\n",
    "from sklearn.model_selection import train_test_split\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences, labels, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "8861b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<750x1339 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3864 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting sentences to vectors using Bag of words model with CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv.fit(sentences_train)\n",
    "\n",
    "vc_traindata = cv.transform(sentences_train)\n",
    "vc_testdata  = cv.transform(sentences_test)\n",
    "vc_traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3ad8071a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(vc_traindata, labels_train)\n",
    "print(\"Accuracy:\", lr_clf.score(vc_testdata, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c1c1b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_clf1 = MultinomialNB()\n",
    "nb_clf1.fit(vc_traindata, labels_train)\n",
    "print(\"Accuracy:\", nb_clf1.score(vc_testdata, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83188175",
   "metadata": {},
   "source": [
    "### Using Deep Neural Network in Keras to classify text\n",
    "You should have Keras and TensorFlow installed on your computer.\n",
    "Once you have Anaconda Python distribution installed, you can install TensorFlow and Keras from Jupyter Notebook by running below code.\n",
    "\n",
    "<code>\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72179c7d",
   "metadata": {},
   "source": [
    "### First we will use Keras Tokenizer to compute word embeddings for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9643e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1626\n"
     ]
    }
   ],
   "source": [
    "# Using Keras Tokenizer to compute embeddings for text\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['sentence'])\n",
    "\n",
    "# Take the sentence text in a variable X and labels in y.\n",
    "X = df['sentence']\n",
    "y = df['label']\n",
    "\n",
    "#Splitting X and y into train and test portions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "#converting sentences into vector sequences\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#Total number of unique words in our sentences is the vocabulary size\n",
    "#Tokenizer.word_index is a python dictionary that contains token keys (string) and token ID values (integer),\n",
    "#and the first token ID is 1 (not zero) and the token IDs are assigned incrementally. \n",
    "# So the greatest token ID in word_index is len(word_index). \n",
    "#Therefore, we need vocabulary of size len(word_index) + 1 to be able to index up to the greatest token ID\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "print('Vocabulary size:' , vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "645a2357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         way plug u unless go converter\n",
       "1                              good case excellent value\n",
       "2                                          great jawbone\n",
       "3      tied charger conversation lasting minutesmajor...\n",
       "4                                              mic great\n",
       "                             ...                        \n",
       "995             screen get smudged easily touch ear face\n",
       "996                           piece junk lose call phone\n",
       "997                                   item match picture\n",
       "998                 thing disappoint infra red port irda\n",
       "999                        answer call unit never worked\n",
       "Name: sentence, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "7ed4b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way: 133\n",
      "conversation: 186\n",
      "easily: 243\n",
      "ear: 11\n"
     ]
    }
   ],
   "source": [
    "# printing the numerical value tokenizer has assigned to these four words in our reviews\n",
    "for word in ['way', 'conversation', 'easily', 'ear']:\n",
    "     print('{}: {}'.format(word, tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "94df73d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[282  31 301   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# padding vector sequences of sentences to make them all of same length\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "maxlen = 50\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "print(X_train[4, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951600bf",
   "metadata": {},
   "source": [
    "#### Using embedding layer of Keras\n",
    "\n",
    "We will now use the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. It requires following parameters: \n",
    "- input_dim: the size of the vocabulary\n",
    "- output_dim: the size of the dense vector\n",
    "- input_length: the length of the sequence\n",
    "\n",
    "We can take the output of the embedding layer and plug it into a Dense layer. During training, the number of parameters to train is <b>vacab_size multiplied by the embedding_dim</b>. The weights of the embedding layer are initialized randomly and then fine tuned using backpropagation during training. This model takes the words as they come in the order of the sentences as input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "52dba012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 50, 100)           162600    \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 100)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 15)                1515      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,131\n",
      "Trainable params: 164,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building a deep neural network model to text classification in Keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 100\n",
    "maxlen = 50\n",
    "model = Sequential()\n",
    "\n",
    "#The embedding layer takes the integer-encoded input and looks up an embedding vector for each word-index. \n",
    "#These vectors are learned as the model trains.\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "\n",
    "#The GlobalMaxPool1D layer returns a fixed-length output vector for each example by applying \n",
    "#Global max pooling operation to input sequences. \n",
    " \n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "# Densely connected hidden layer with 15 units and activation function relu\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "\n",
    "# Output layer with activation sigmoid.\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "16fc6c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9973\n",
      "Testing Accuracy:  0.7760\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy curve for training and validation data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(9, 5))\n",
    "\n",
    "history = model.fit(X_train, y_train,epochs=50, verbose=False,validation_data=(X_test, y_test), batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "x = range(1, len(acc) + 1)\n",
    "plt.plot(x, acc, 'b', label='Training acc')\n",
    "plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "plt.legend()\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a6482",
   "metadata": {},
   "source": [
    "# Transformers - GPT3, BERT, GPT4 ...\n",
    "\n",
    "- In Electrical Engineering, a transformer is an electrical device that uses the principle of electromagnetic induction to transfer energy from one electric circuit to another. \n",
    "\n",
    "<img src = 'transformersinelectricity.png'>\n",
    "<center>Source:https://www.fierceelectronics.com/electronics/what-a-transformer </center>\n",
    "\n",
    "- In Computer Science, transformers are a deep neural network archtecture used to transform input sequences into output sequences e.g., in tasks like machine translation.\n",
    "\n",
    "\n",
    "There have been so many transformer models since GPT3. \n",
    "\n",
    "<img src= 'modelssincegpt3.jpg'>\n",
    "<center>Source: https://twitter.com/nazneenrajani/status/1636201602406047746 </center>\n",
    "\n",
    "## Transformer Models\n",
    "\n",
    "- <b>Transformers are a deep neural network archtecture</b> introduced in 2017 by a team at Google Brain.\n",
    "- Transformer uses <b>self-attention mechanism</b> to differentially weigh the important parts of input. Attention mechanism provides context for any position in the input sequence.\n",
    "- Transformers process sequential data just like RNNS, however unlike RNNs, transformers can process entire input at once instead of one word at a time.  \n",
    "- Transformer can be trained parallely, hence enormous datasets can be used as input. This has enabled pretrained transformer models like Google BERT(Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer) etc. These are trained on Internet data like Wikipedia and Common Crawl and can be fine tuned.\n",
    "- NLP and computer vision fields have benefitted immensely by transformer architecture.\n",
    "\n",
    "<img src = 'transformers.png'>\n",
    "<center>Source:https://blog.paperspace.com/transformers-text-classification/</center>\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- The components that make transformers very powerful as compared to conventional neural networks like CNN and RNN are:\n",
    "\n",
    " 1. <b>Positional Encodings :</b> The sequential data, e.g., words in a sentence, is fed into the transformer such that the position of the word in the sentence is also a part of the input. \n",
    "     e.g., The sentence : 'I love roses' will be fed like ('I', 1), ('love', 2), ('roses',3). \n",
    "      Where the number indicates the order of the word in the sentence. The positional encodings are generated using a sine    function which uses a different formula for words are even and odd positions in text.\n",
    "\n",
    "2. <b> Attention Mechanism including Self-attention: </b>Attention mechanism was introduced in 2015 in (https://arxiv.org/pdf/1409.0473.pdf). It allows a model to 'attend to' or 'look at' every single word in input sentence to decide about how to replace that word in the output sentence in tasks like machine translation. The example from the original paper helps explain this:\n",
    "\n",
    "    - English Sentence :                 The agreement on the European Economic Area was signed in August 1992.\n",
    "    - Corresponding French Translation:  Laccord sur la zone conomique europenne a t sign en aot 1992.\n",
    "\n",
    "We know that different languages follow different rules regarding use of word order, gender words etc. So it doesn't produce correct translation if model just replaces each word in original sentence with corresponding word in destination language (English to French here).  E.g., some words in the French translation are flipped: its European Economic Area in English, but la zone conomique europenne in French. Also, French is a language with gendered words. The adjectives conomique and europenne must be in feminine form to match the feminine object la zone.\n",
    "\n",
    "<img src = 'whichwordstofocuson.png' width = 1200 height = 1000>\n",
    "<center>Source: Neural machine translation by jointly learning to align and translate https://arxiv.org/pdf/1409.0473.pdf </center>\n",
    "\n",
    "The model learns these rules (e.g., about gender, grammar, plurality) of different languages and hence it learns about which words it should be attending to at each time step from the training data that consists of huge volumes of sentences in the source and destination languages.  \n",
    "\n",
    "\n",
    "<b>Self-Attention</b>\n",
    "\n",
    "- While general attention mechanism is used to align words across different source and destination languages during machine translation, self-attention helps build a model to understand a specific language. It enables a transformer model to learn about the training data the information like POS tags, grammar rules, relationships between words (e.g., synonyms), entity resolution etc.\n",
    "\n",
    "- For example, in sentences below the word 'dish' has different meanings.\n",
    "1. 'I like this porcelain dish'\n",
    "3. 'I like this rice dish'\n",
    "2. 'Dish out your gifts'\n",
    "\n",
    "- Self-attention mechanism enables the model to disambiguate the meaning of these words by paying attention to context words like 'porcelain' , 'rice', and 'out' in these 3 sentences. Which words to attend to is learnt during training on large language corpora. \n",
    "\n",
    "- Self-attention in an encoder learns associations within words of input sequence. Transformers use positional encoding to tag words in an sequence. Attention units use these tags to compute an algebraic map of relationships between each element of input sequence.\n",
    "\n",
    "\n",
    "#### Encoder Decoder Architecture\n",
    "\n",
    "- Transformers use an encoder-decoder architecture. They can have multiple blocks of encoders and decoders. \n",
    "\n",
    "- Input: The input text is parsed into tokens by a tokenizer, and each token is converted via a word embedding into a vector. Then, positional information of the token is added to the word embedding.\n",
    "\n",
    "- Positional encoding: A positional encoding is a fixed-size vector representation that encapsulates the relative positions of tokens within a target sequence. It provides the transformer model with information about where the words are in the input sequence.\n",
    "\n",
    "- Encoder : Each encoder has two parts: self-attention layer and feed forward neural net. The first encoder takes positional information and embeddings of the input sequence as its input and process them with the self-attention layer before passing it to the feed forward neural net. \n",
    "\n",
    "- For every next encoder, the self-attention mechanism accepts input encodings from the previous encoder and weighs their relevance to each other to generate output encodings. The feed-forward neural network further processes each output encoding individually. These output encodings are then passed to the next encoder as its input, as well as to the decoders. The final encoders output is passed to the decoders.\n",
    "\n",
    "- The encoder is bidirectional. Attention can be placed on tokens before and after the current token.\n",
    "\n",
    "- In earlier deep models like RNN or LSTM, only last hidden state was passed to the decoder as context, but in transformers with attention, all of previous states(for previous words) are passed to decoder as context. \n",
    "\n",
    "- Decoder : Decoder calculates importance of each hidden state received from encoder using its own output of the previous time step. Higher importance is given to hidden states with higher weights in the next context vector. Then this context is used as input to decoder's feed forward layer to get the output sequence. This output sequence becomes the current hidden state of decoder and is used to calculate weights of next time step.\n",
    "\n",
    "- Masked multi-head attention in decoder ensures that each consecutive output only has knowledge of the previous input.\n",
    "\n",
    "- Each decoder has three parts: Self-attention layer, decoder-attention layer over encodings and feed forward neural net. An additional attention mechanism is present in decoder which draws relevant information from the encodings generated by the encoders. \n",
    "\n",
    "- Each decoder layer does the opposite of what encoder does. It takes in all the encodings and uses their incorporated contextual information to generate an output sequence. Decoder-attention mechanism enables it to draw information from the outputs of previous decoders, before the decoder layer draws information from the encodings.\n",
    "\n",
    "- Both the encoder and decoder layers have a feed-forward neural network for additional processing of the outputs and contain residual connections and layer normalization steps\n",
    "\n",
    "- The embedding input in the form of vectors, keys, and queries is passed through a linear layer.\n",
    "\n",
    "\n",
    "#### Applications of Transformers \n",
    "(https://openai.com/blog/gpt-3-apps)\n",
    "\n",
    "- Machine Translation\n",
    "- Computer Vision\n",
    "- Conversational AI\n",
    "- Auto completion\n",
    "- Search\n",
    "- Text Generation\n",
    "- Text Summarization\n",
    "- Conversations\n",
    "- Text Classification\n",
    "- Named Entity Resoltion\n",
    "- Computational Biology: Protein Structure prediction with AlphaFold in 2021, Drug Discovery.\n",
    "\n",
    "#### How to use transformers\n",
    "Frameworks like TensorFlow, PyTorch (PyTorch-Transformers), Keras, HuggingFace provide you with transformer models BERT, RoBERTa, XLNetT5, GPT2 etc. to use in your applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffe5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
